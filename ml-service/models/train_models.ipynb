{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa096ca",
   "metadata": {},
   "source": [
    "### Agri Sathi — AI Model Training Notebook\n",
    "### Run this entire notebook on Google Colab (CPU runtime is fine for soil model, use GPU for pest model)\n",
    "### After training, download the model files and add them to the ml-service/weights/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1 — Install dependencies\n",
    "!pip install tensorflow==2.13.0 scikit-learn xgboost kaggle Pillow numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30bbe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 — Download datasets from Kaggle\n",
    "# First upload your kaggle.json API key to Colab\n",
    "from google.colab import files\n",
    "files.upload()  # Upload kaggle.json\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Dataset 1: PlantVillage for pest detection\n",
    "!kaggle datasets download -d abdallahalidev/plantvillage-dataset\n",
    "!unzip -q plantvillage-dataset.zip -d plantvillage\n",
    "\n",
    "# Dataset 2: Crop Recommendation\n",
    "!kaggle datasets download -d atharvaingle/crop-recommendation-dataset\n",
    "!unzip -q crop-recommendation-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc433e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file '.venv\\Lib\\site-packages\\typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# CELL 3 — SOIL MODEL (Train this first — it's fast, CPU-friendly, takes ~2 minutes)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('model\\Crop_recommendation.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Crops: {df['label'].unique()}\")\n",
    "print(f\"Class distribution:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "# Features and target\n",
    "FEATURES = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "TARGET = 'label'\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"\\nLabel mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "print(f\"\\nTrain size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "\n",
    "# Train XGBoost (primary model)\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "# Train Random Forest (backup model)\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate both\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nXGBoost Accuracy: {accuracy_score(y_test, xgb_pred):.4f}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nXGBoost Classification Report:\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=le.classes_))\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(f\"\\nFeature Importances:\\n{importance_df}\")\n",
    "\n",
    "# Cross validation\n",
    "cv_scores = cross_val_score(xgb_model, X, y_encoded, cv=5, scoring='accuracy')\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Save model artifacts\n",
    "soil_artifacts = {\n",
    "    'model': xgb_model,\n",
    "    'label_encoder': le,\n",
    "    'features': FEATURES,\n",
    "    'accuracy': float(accuracy_score(y_test, xgb_pred)),\n",
    "    'n_classes': len(le.classes_),\n",
    "    'class_names': list(le.classes_),\n",
    "}\n",
    "\n",
    "with open('soil_model.pkl', 'wb') as f:\n",
    "    pickle.dump(soil_artifacts, f)\n",
    "\n",
    "# Save class names as JSON for the API\n",
    "with open('soil_classes.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'classes': list(le.classes_),\n",
    "        'features': FEATURES,\n",
    "        'accuracy': float(accuracy_score(y_test, xgb_pred))\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Soil model saved as soil_model.pkl\")\n",
    "print(f\"✅ soil_classes.json saved with {len(le.classes_)} crop classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d66080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 — PEST DETECTION MODEL (Requires GPU runtime for reasonable speed)\n",
    "# Runtime → Change runtime type → GPU before running this cell\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Dataset path — adjust if PlantVillage extracted to different folder\n",
    "DATASET_PATH = 'plantvillage/plantvillage dataset/color'\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Count classes\n",
    "classes = [d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))]\n",
    "NUM_CLASSES = len(classes)\n",
    "print(f\"\\nFound {NUM_CLASSES} disease classes\")\n",
    "print(f\"Classes: {sorted(classes)[:10]}... (showing first 10)\")\n",
    "\n",
    "# Data generators with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")\n",
    "print(f\"Classes found: {len(train_generator.class_indices)}\")\n",
    "\n",
    "# Save class indices mapping (critical for inference)\n",
    "class_indices = train_generator.class_indices\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "with open('pest_class_labels.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'class_indices': class_indices,\n",
    "        'idx_to_class': idx_to_class,\n",
    "        'num_classes': NUM_CLASSES\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ pest_class_labels.json saved\")\n",
    "\n",
    "# Build model\n",
    "print(\"\\nBuilding MobileNetV2 model...\")\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False  # Freeze base initially\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint('pest_model_best.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "]\n",
    "\n",
    "# Phase 1: Train classification head only (frozen base)\n",
    "print(\"\\n=== PHASE 1: Training classification head (frozen base) ===\")\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 1 best validation accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Phase 2: Fine-tune top layers of base model\n",
    "print(\"\\n=== PHASE 2: Fine-tuning top 30 layers of MobileNetV2 ===\")\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_count = sum(1 for layer in model.layers if layer.trainable)\n",
    "print(f\"Trainable layers: {trainable_count}\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Much lower LR for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
    ")\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n=== Final Evaluation ===\")\n",
    "val_loss, val_accuracy, val_top3 = model.evaluate(val_generator, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(f\"Top-3 Accuracy: {val_top3:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "model.save('pest_model.h5')\n",
    "print(\"\\n✅ pest_model.h5 saved\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'MobileNetV2',\n",
    "    'input_size': 224,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'val_accuracy': float(val_accuracy),\n",
    "    'val_top3_accuracy': float(val_top3),\n",
    "    'training_date': pd.Timestamp.now().isoformat(),\n",
    "}\n",
    "with open('pest_model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17accaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 — Download all model files\n",
    "from google.colab import files\n",
    "files.download('pest_model.h5')          # ~14MB\n",
    "files.download('soil_model.pkl')          # ~2MB\n",
    "files.download('pest_class_labels.json')\n",
    "files.download('soil_classes.json')\n",
    "files.download('pest_model_metadata.json')\n",
    "\n",
    "print(\"\\n✅ All model files downloaded. Place them in ml-service/weights/ folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
